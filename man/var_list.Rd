% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/var_list.R
\name{var_list}
\alias{var_list}
\title{Create a Variable List}
\usage{
var_list(topic,
         include_topic = FALSE,
         n_final = Inf,
         n_variables = "all",
         LLM_model = "gpt-4o",
         max_tokens = 2000,
         update_key = FALSE)
}
\arguments{
\item{topic}{A character vector specifying the topic for which a theory should be developed. If it is not feasible to identify a particular topic, the argument can be set to NULL.}

\item{include_topic}{If \code{include_topic = FALSE} (default), the topic specified in the \code{"topic"} argument will not be included as a seperate variable in the variable list.}

\item{n_final}{Number of variables to be included in the final variable list. If \code{inf} (default), the final integrated variable list will not be limited to a certain number of variables.}

\item{n_variables}{Number of variables the LLM should generate in the first 2 variable lists. If \code{"all"} (default), the LLM is not limited to generate a specific number of variables, but is asked to create a list of "all" important variables.}

\item{LLM_model}{The LLM model that should be used to generate output: \code{"gpt-4o"} (default), \code{"gpt-4"}, \code{"gpt-4-turbo"} \code{"gpt-3.5-turbo"}, \code{"llama-3"} (specifically points to Llama-3-70B-Chat-hf, accessed via Hugging Face), or \code{"mixtral"} (specifically points to Mixtral-8x7B-Instruct-v0.1, accessed via Together.ai).}

\item{max_tokens}{The maximum number of tokens the LLM should generate. Be careful when adjusting this argument. Reducing the maximum token limit will reduce the cost but may result in incomplete answers. Conversely, increasing the token limit can be advantageous for obtaining more detailed responses. The maximum number of tokens depends on the model (\code{6000} for \code{"gpt-4o"}, \code{"gpt-4"}, and \code{"gpt-4-turbo"}, \code{3000} for \code{"gpt-3.5-turbo"}, and \code{2000} for \code{"mixtral"}).}

\item{update_key}{If \code{update_key = TRUE}, the function will prompt the user for a new API key and update the saved key. If \code{update_key = FALSE} (default), the function will use the existing API key if available.}
}
\value{
\itemize{
  \item \code{raw_LLM}: A dataframe containing the unprocessed LLM output along with some other LLM information, including:
    \itemize{
      \item \code{function_part}: Part of function from which LLM output originates.
      \item \code{iteration}: Iteration number.
      \item \code{LLM_model}: LLM model used.
      \item \code{prompt}: Prompt used.
      \item \code{system_prompt}: System prompt used.
      \item \code{content}: Unprocessed LLM output.
      \item \code{finish_reason}: Reason the LLM stopped generating output.
      \item \code{prompt_tokens}: Number of tokens used for the LLM prompt.
      \item \code{answer_tokens}: Number of tokens used for the LLM answer.
      \item \code{total_tokens}: Total number of tokens used.
      \item \code{error}: Error message, if any occurred.
    } \cr
  \item \code{all_vars}: A vector containing the integrated variable list with every variable generated by the LLM.
  \item \code{final_list}: A vector containing the final variable list. If the n_final argument is set in Inf or a number higher than the number or variables in all_vars the final list will be exactly the same as all_vars.
}
}
\description{
In \code{var_list()} a Large Language Model (LLM) is instructed to generate a list of important variables for a particular topic.

The function prompts a LLM to compile two lists of variables with 2 different prompts.
The LLM is instructed to include only variables that are precisely defined and measured on a binary or continuous scale.
The LLM then merges the two lists, removing duplicates and synonyms, and reviews the combined list to identify any missing crucial variables.
Finally, it refines the list by separating combined factors into distinct items.

In addition, users can limit the final number of variables, prompting the LLM to select the N most important variables.
}
\details{
To create a theory from scratch, the functions in this R-package should be used in the following order:

\code{\link{var_list}} --> \code{\link{causal_relation}} --> \code{\link{causal_direction}} --> \code{\link{causal_sign}} --> \code{\link{cld_plot}} --> \code{\link{find_source}}
}
\note{
The function and its output should be approached with caution. Depending on the specific LLM used, there may be financial implications. Furthermore, we wish to emphasise that the answers generated by an LLM should not be taken as absolute truth.
}
\examples{
\dontrun{
# For a readily available, pre-made output example see: data("vars")
vars <- var_list(topic = "addiction",
                 n_final = 10,
                 n_variables = "all")

# Check output
vars$all_vars
vars$final_list
}

}
\references{
\url{https://platform.openai.com}
}
\seealso{
\code{\link{cld}},
\code{\link{causal_relation}},
\code{\link{causal_direction}},
\code{\link{causal_sign}},
\code{\link{cld_plot}},
\code{\link{find_source}}
}
\author{
Meike Waaijers
}
