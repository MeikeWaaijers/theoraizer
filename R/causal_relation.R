# Program Name: theoraizer
# Description: In the causal_relation function a Large Language Model (LLM) is asked to indicate whether there is a causal or non-causal relationship between variable pairs.
# Copyright (C) <2024> <Meike Waaijers>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.


#### theoraizer
### Causal relation function

## Function manual

#' Identify the Presence of Causal Relationships
#'
#' @description
#' \code{causal_relation()} will generate all possible unique pairs of variables (e.g. 4 variables will give 5 * (5 - 1) / 2 = 10 unique pairs). It will then iterate through all these pairs and ask a Large Language Model (LLM) to indicate whether there is a causal or non-causal relationship between the variables.
#'
#' @usage
#' causal_relation(topic,
#'                 variable_list,
#'                 LLM_model = "gpt-4o",
#'                 max_tokens = 2000,
#'                 update_key = FALSE)
#' @details
#' To create a fully fledged theory from scratch, the functions in this R-packaged should be used in the following order:
#'
#' \code{\link{var_list}} --> \code{\link{causal_relation}} --> \code{\link{causal_direction}} --> \code{\link{causal_sign}} --> \code{\link{cld_plot}}
#'
#' @param topic A character vector specifying the topic for which a theory should be developed. If it is not feasible to identify a particular topic, the argument can be set to NULL.
#' @param variable_list A vector containing all variables that need to be included in the theory.
#' @inheritParams var_list
#'
#' @returns
#' \itemize{
#'   \item \code{raw_LLM}: A dataframe containing the unprocessed LLM output along with some other LLM information, including:
#'     \itemize{
#'       \item \code{relationship}: Which variable pair.
#'       \item \code{iteration}: Iteration number.
#'       \item \code{LLM_model}: LLM model used.
#'       \item \code{prompt}: Prompt used.
#'       \item \code{system_prompt}: System prompt used.
#'       \item \code{content}: Unprocessed LLM output.
#'       \item \code{finish_reason}: Reason the LLM stopped generating output.
#'       \item \code{prompt_tokens}: Number of tokens used for the LLM prompt.
#'       \item \code{answer_tokens}: Number of tokens used for the LLM answer.
#'       \item \code{total_tokens}: Total number of tokens used.
#'       \item \code{error}: Error message, if any occurred.
#'     }
#' } \cr
#' \itemize{
#'   \item \code{relation_df}: A dataframe with three columns:
#'     \itemize{
#'       \item \code{var1}: Variable 1 of a unique variable pair.
#'       \item \code{var2}: Variable 2 of a unique variable pair.
#'       \item \code{prob_causal}: Probability of the presence of a causal relationship between var1 and var2.
#'     }
#' }
#' @references \url{https://platform.openai.com}
#'
#' @author Meike Waaijers
#'
#' @note The function and its output should be approached with caution. Depending on the specific LLM used, there may be financial implications. Furthermore, we wish to emphasise that the answers generated by an LLM should not be taken as absolute truth.
#'
#' @seealso
#' \code{\link{cld}},
#' \code{\link{var_list}},
#' \code{\link{causal_direction}},
#' \code{\link{causal_sign}},
#' \code{\link{cld_plot}},
#'
#' @examples
#' \dontrun{
#' ## Example input (topic = "addiction")
#' data("vars")
#' vars$final_list
#'
#' #---------------------------------------------------------------------------
#' ## Default
#' # For a readily available, pre-made output example see: data("rel")
#' rel <- causal_relation(topic = "addiction",
#'                        variable_list = vars$final_list)
#'
#' # Check output
#' rel$relation_df
#' }
#' @import httr
#' @import utils
#' @import keyring
#' @export


## causal_relation function
causal_relation <- function(topic,
                            variable_list,
                            LLM_model = "gpt-4o",
                            max_tokens = 2000,
                            update_key = FALSE) {

  #validate input
  stopifnot("'topic' should be a character string or NULL." = is.character(topic) | is.null(topic))
  stopifnot("'variable_list' should be a vector containing more than one variables." = is.vector(variable_list) && length(variable_list) > 1)
  stopifnot("All entries in 'variable_list' should be character strings." =
              all(sapply(variable_list, is.character)))
  stopifnot("'LLM_model' should be 'gpt-4o', 'gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo', 'mixtral', or 'llama-3'." =
              LLM_model %in% c("mixtral", "gpt-4o", "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", "llama-3"))
  stopifnot("For 'gpt-4o', 'max_tokens' should be a whole number above 0, and not higher than 6000." =
              !(LLM_model == "gpt-4o") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 6000))
  stopifnot("For 'gpt-4', 'max_tokens' should be a whole number above 0, and not higher than 6000." =
              !(LLM_model == "gpt-4") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 6000))
  stopifnot("For 'gpt-4-turbo', 'max_tokens' should be a whole number above 0, and not higher than 6000." =
              !(LLM_model == "gpt-4-turbo") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 6000))
  stopifnot("For 'gpt-3.5-turbo', 'max_tokens' should be a whole number above 0, and not higher than 3000." =
              !(LLM_model == "gpt-3.5-turbo") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 3000))
  stopifnot("For 'mixtral', 'max_tokens' should be a whole number above 0, and not higher than 2000." =
              !(LLM_model == "mixtral") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 2000))
  stopifnot("For 'llama-3', 'max_tokens' should be a whole number above 0, and not higher than 6000." =
              !(LLM_model == "llama-3") || (is.numeric(max_tokens) && max_tokens == floor(max_tokens) && max_tokens >= 0 && max_tokens <= 6000))
  stopifnot("'update_key' should be a logical value." = is.logical(update_key))

  ## Load and prepare prompt data
  prompt_file_path <- system.file("extdata", "prompts.csv", package = "theoraizer")
  prompts_data <- utils::read.csv(prompt_file_path, sep = ";")

  # replace '\\n' with '\n' in all text columns
  for (i in 1:length(prompts_data$Prompt)) {
    # Check if the prompt column is a character type
    if (is.character(prompts_data$Prompt[[i]])) {
      # Replace '\\n' with '\n' in the column
      prompts_data$Prompt[[i]] <- gsub("\\n", "\n",
                                       prompts_data$Prompt[[i]],
                                       fixed = TRUE)
    }

    # Check if the sys prompt column is a character type
    if (is.character(prompts_data$Sys.Prompt[[i]])) {
      # Replace '\\n' with '\n' in the column
      prompts_data$Sys.Prompt[[i]] <- gsub("\\n", "\n",
                                           prompts_data$Sys.Prompt[[i]],
                                           fixed = TRUE)
    }
  }

  rel_prompts <- prompts_data[prompts_data$Function == "relations", ]


  ## Create objects for tryCatch output
  # So somethings gets outputted even though an error occurs later on in the function
  raw_LLM <- NULL
  raw_LLM_prompt <- NULL
  logprobs_LLM <- NULL
  logprobs_LLM_prompt <- NULL
  prob_relation_df <- NULL

  pairs_df <- data.frame(var1 = character(), var2 = character())

  # Generate all unique combinations
  for(i in 1:(length(variable_list)-1)) {
    for(j in (i+1):length(variable_list)) {
      pairs_df <- rbind(pairs_df, data.frame(var1 = variable_list[[i]], var2 = variable_list[[j]]))
    }
  }

  # How many different pairs
  n_pairs <- dim(pairs_df)[1]

  #LLM
  for (i in 1:n_pairs) {
    message(sprintf("Variable pair: %d / %d", i, n_pairs))

    # Initialize the prompt database
    prompt_database <- list()

    for (g in 1:2) {
      if (length(prompt_database) == 0) {
        if (is.null(topic)) {
          # Create optional prompts
          prompt1 <- gsub("\\((pairs_df\\[i, 1\\])\\)", pairs_df[i, 1],
                          gsub("\\((pairs_df\\[i, 2\\])\\)", pairs_df[i, 2],
                               rel_prompts$Prompt[1]))

          prompt2 <- gsub("\\((pairs_df\\[i, 2\\])\\)", pairs_df[i, 2],
                          gsub("\\((pairs_df\\[i, 1\\])\\)", pairs_df[i, 1],
                               rel_prompts$Prompt[2]))

        } else {
          # Create optional prompts
          prompt1 <- gsub("\\((pairs_df\\[i, 1\\])\\)", pairs_df[i, 1],
                          gsub("\\((pairs_df\\[i, 2\\])\\)", pairs_df[i, 2],
                               gsub("\\((topic)\\)", topic,
                                    rel_prompts$Prompt[3])))

          prompt2 <- gsub("\\((pairs_df\\[i, 2\\])\\)", pairs_df[i, 2],
                          gsub("\\((pairs_df\\[i, 1\\])\\)", pairs_df[i, 1],
                               gsub("\\((topic)\\)", topic,
                                    rel_prompts$Prompt[4])))

        }

        # Put all prompts in a database
        prompt_database <- list(prompt1, prompt2)

      }

      prompt <- prompt_database[[g]]

      ## LLM
      # Create system prompt
      system_prompt <- rel_prompts$Sys.Prompt[1]

      # LLM
      LLM_output <- LLM(prompt = prompt,
                        LLM_model = LLM_model,
                        max_tokens = ifelse(LLM_model == "mixtral", 4, max_tokens),
                        temperature = 0, # = how creative LLM can be (0 = not creative at all so the same answer will be given if you run the exact same prompt again.)
                        logprobs = TRUE,
                        raw_output = TRUE,
                        system_prompt = system_prompt,
                        update_key = update_key)

      update_key <- FALSE # make sure api key is only updated once
      raw_LLM_prompt[[g]] <- c(prompt = prompt, system_prompt = system_prompt, LLM_output$raw_content)
      logprobs_LLM_prompt[[g]] <- LLM_output$top5_tokens
    }

    raw_LLM[[i]] <- raw_LLM_prompt
    logprobs_LLM[[i]] <- logprobs_LLM_prompt

  }


  #tryCatch in case processing steps fail the raw output will still be outputted
  tryCatch({

    if (LLM_model == "mixtral" | LLM_model == "llama-3"){
      last_token <- NULL
      for (i in 1:n_pairs) {
        last_token_t <- NULL
        for (j in 1:length(logprobs_LLM[[i]])){
          last_token_t[[j]] <- logprobs_LLM[[i]][[j]][[1]]
          last_token_t[[j]]$top5_tokens <- trimws(tolower(last_token_t[[j]]$top5_tokens))
        }
        last_token[[i]] <- last_token_t
      }

    } else {
      last_token <- NULL
      for (i in 1:n_pairs) {
        last_token_t <- NULL
        for (j in 1:length(logprobs_LLM[[i]])){
          last_token_t[[j]] <- logprobs_LLM[[i]][[j]][[length(logprobs_LLM[[i]][[j]])]]
          last_token_t[[j]]$top5_tokens <- trimws(tolower(last_token_t[[j]]$top5_tokens))
        }
        last_token[[i]] <- last_token_t
      }

    }

    all_prob <- list()
    valid_tokens <- c("c", "n")

    for (l in 1:length(last_token)) {
      probs <- list()
      for (g in 1:length(last_token[[l]])){
        class <- NULL
        prob <- NULL
        k <- 1
        # Process each item in last_token
        for (m in 1:nrow(last_token[[l]][[g]])) {
          if (trimws(tolower(last_token[[l]][[g]]$top5_tokens[m])) %in% valid_tokens) {
            # Filter and extract required values
            class[k] <- trimws(tolower(last_token[[l]][[g]]$top5_tokens[m]))
            prob[k] <- trimws(tolower(last_token[[l]][[g]]$probability[m]))
          } else {
            class[k] <- "-"
            prob[k] <- "-"
          }
          k <- k + 1
        }
        # Filter positive probabilities
        positive_probs <- prob > 0 | prob == "-"
        probs[[g]] <- data.frame(Class = class[positive_probs], Probability = prob[positive_probs])
      }
      all_prob[[l]] <- probs
    }

    # probabilities for the c class
    c_class <- NULL
    c_class_t <- NULL
    for (i in 1:n_pairs) {
      c_class_t <- 0
      for (k in 1:2){
        for (j in seq_along(all_prob[[i]][[k]]$Class)) {
          if (trimws(tolower(all_prob[[i]][[k]]$Class[j])) == "c") {
            c_class_t[k] <- as.numeric(all_prob[[i]][[k]]$Probability[j])
          }
        }
      }
      c_class[i] <- ifelse(max(c_class_t) > 100, round(max(c_class_t)), round(max(c_class_t), 2))
    }

    # replace na's with 0
    c_class[is.na(c_class)] <- 0

    prob_relation_df <- data.frame(var1 = pairs_df[, 1] , var2 = pairs_df[, 2], prob_causal = c_class, row.names = NULL)

  }, error = function(e) {
    cat(paste0("Warning: Unable to process LLM output -> ", e$message, "."),
        "Only part of the output is returned.", sep = "\n")
  })

  # Initialize the output list
  output <- list()

  # Add raw_LLM to output

  tryCatch({
    # Initialize empty dataframe
    flattened_df_raw_LLM <- data.frame(iteration = integer(),
                                       LLM_model = character(),
                                       prompt = character(),
                                       system_prompt = character(),
                                       content = character(),
                                       finish_reason = character(),
                                       prompt_tokens = numeric(),
                                       answer_tokens = numeric(),
                                       total_tokens = numeric(),
                                       error = character(),
                                       stringsAsFactors = FALSE)



    # Flatten raw_LLM
    for (i in seq_along(raw_LLM)) {
      for (j in seq_along(raw_LLM[[i]])) {
        temp <- raw_LLM[[i]][[j]]
        flattened_df_raw_LLM <- rbind(flattened_df_raw_LLM,
                                      data.frame(relationship = i,
                                                 iteration = j,
                                                 LLM_model = temp$LLM_model,
                                                 prompt = temp$prompt,
                                                 system_prompt = temp$system_prompt,
                                                 content = temp$content,
                                                 finish_reason = temp$finish_reason,
                                                 prompt_tokens = temp$prompt_tokens,
                                                 answer_tokens = temp$answer_tokens,
                                                 total_tokens = temp$total_tokens,
                                                 error = ifelse(is.null(temp$error), NA, temp$error),
                                                 stringsAsFactors = FALSE))

      }
    }
    output$raw_LLM <- flattened_df_raw_LLM
  }, error = function(e) {
    cat(paste0("Warning: Unable to return raw LLM output -> ", e$message, "."),
        "Only part of the output is returned.", sep = "\n")

  })


  # Adding prob_relation_df to output
  output$relation_df <- prob_relation_df


  message(sprintf("Total of LLM prompts: %d", n_pairs * 2))

  # give openai error if there is no output at all
  if (length(output) == 0) {
    for (i in 1:n_pairs) {
      if (!is.null(raw_LLM[[i]][[1]]$error$message)) {
        stop(raw_LLM[[i]][[1]]$error$message)
      } else if (!is.null(raw_LLM[[i]][[2]]$error$message)) {
        stop(raw_LLM[[i]][[2]]$error$message)
      }
    }
  }

  return(output)
}
